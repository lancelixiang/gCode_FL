{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_TO_INDEX = {'0': 0, '3': 1, '4': 2, '5': 3}\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# some regions are duplicates and should not need be included\n",
    "DUPLICATES = [\n",
    "    '16B0001851_Block_Region_3',\n",
    "    '16B0003388_Block_Region_5',\n",
    "    '16B0003394_Block_Region_1',\n",
    "    '16B0022608_Block_Region_2',\n",
    "    '16B0022786_Block_Region_0',\n",
    "    '16B0023614_Block_Region_3',\n",
    "    '16B0026792_Block_Region_3',\n",
    "    '16B0027040_Block_Region_10',\n",
    "    '18B0005478J_Block_Region_13',\n",
    "    '18B0005478J_Block_Region_10'\n",
    "]\n",
    "\n",
    "# 定义图像预处理步骤\n",
    "PREPROCESS = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(model, paths, type='mobileNet'):\n",
    "    feature_arr = []\n",
    "    \n",
    "    length = math.ceil(len(paths)/BATCH_SIZE)\n",
    "    for i in range(length):\n",
    "        batchArr = []\n",
    "        end = min(len(paths), i*BATCH_SIZE + BATCH_SIZE)\n",
    "        for path in paths[i*BATCH_SIZE:end]:\n",
    "            region = '_'.join(path.split('/')[-1].split('_')[0:4])\n",
    "            if (region) in DUPLICATES:\n",
    "                print('.................DUPLICATES...................', path)\n",
    "                # continue\n",
    "\n",
    "            # 加载图像并进行预处理\n",
    "            img = Image.open(path)\n",
    "            img_tensor = PREPROCESS(img)\n",
    "\n",
    "            # 添加批次维度，并移动到设备\n",
    "            batchArr.append(img_tensor.unsqueeze(0).to(DEVICE))\n",
    "            \n",
    "        input_batch = torch.cat(batchArr, dim=0)\n",
    "        \n",
    "        feature = model(input_batch)\n",
    "        feature_arr.append(feature.detach())\n",
    "    \n",
    "    features = torch.cat(feature_arr, dim=0)\n",
    "    # print(f\"{type} features shape: {features.shape}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mobileNet_feature(paths):\n",
    "    # 创建一个新的模型，只到全局平均池化层\n",
    "    # features shape: torch.Size([2, 3, 224, 224])\n",
    "    layer_model = models.mobilenet_v2(pretrained=True)\n",
    "    # layer_model = models.mobilenet_v3_large() # features shape: torch.Size([2, 960, 7, 7])\n",
    "    # layer_model = models.mobilenet_v3_small() # features shape: torch.Size([2, 576, 7, 7])\n",
    "    # layer_model = torch.nn.Sequential(*list(layer_model.children())[:-1])\n",
    "    # print(layer_model)\n",
    "    # print(hasattr(layer_model, 'classifier'))\n",
    "    layer_model.classifier[-1] =  torch.nn.Sequential()\n",
    "    layer_model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        layer_model.to(DEVICE)\n",
    "    \n",
    "    return extract_feature(layer_model, paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ViT_feature(paths):\n",
    "    # 创建一个新的模型，只到全局平均池化层\n",
    "    layer_model = models.vit_l_32(pretrained=True) \n",
    "    # layer_model = models.maxvit_t() # torch.Size([2, 64, 112, 112])\n",
    "    # layer_model = models.vit_b_16() # torch.Size([2, 768, 14, 14])\n",
    "    # layer_model = models.vit_b_32() # torch.Size([2, 768, 7, 7])\n",
    "    # layer_model = models.vit_h_14() # torch.Size([2, 1280, 16, 16])\n",
    "    # layer_model = models.vit_l_16() # torch.Size([2, 1024, 14, 14])\n",
    "    # layer_model = models.vit_l_32() # torch.Size([2, 1024, 7, 7])\n",
    "    # print(layer_model)\n",
    "    # print(hasattr(layer_model, 'heads'))\n",
    "    layer_model.heads[-1] =  torch.nn.Sequential()\n",
    "    # layer_model = torch.nn.Sequential(*list(layer_model.children())[:-1])\n",
    "    layer_model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        layer_model.to(DEVICE)\n",
    "        \n",
    "    return extract_feature(layer_model, paths, type=\"ViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    os.makedirs('dataset/features', exist_ok=True)\n",
    "\n",
    "    ALL_PATH = glob.glob(os.path.join('dataset', 'SICAPv2', 'images', '*.jpg'))\n",
    "    IMAGE_LABELS = pd.read_csv('dataset/image_labels.csv')\n",
    "    for i, row in tqdm(IMAGE_LABELS.iterrows()):\n",
    "        name = row.iloc[0]\n",
    "        gleason_grade = row.iloc[3].split('+')\n",
    "        label = np.zeros(4, dtype=int)\n",
    "        label[LABEL_TO_INDEX[gleason_grade[0]]] = 1\n",
    "        label[LABEL_TO_INDEX[gleason_grade[1]]] = 1\n",
    "\n",
    "        paths = [f for f in ALL_PATH if name in f and '_'.join(f.split('/')[-1].split('_')[0:4]) not in DUPLICATES]\n",
    "        m_features = extract_mobileNet_feature(paths)\n",
    "        v_features = extract_ViT_feature(paths)\n",
    "\n",
    "        # print(i, name, label, m_features.shape, v_features.shape)\n",
    "        data = {\n",
    "            'name': name,\n",
    "            'label': label,\n",
    "            'm_features': m_features,\n",
    "            'v_features': v_features\n",
    "        }\n",
    "        torch.save(data, f'dataset/features/{name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFeature = torch.load('dataset/features/16B0003388.pth', weights_only=False)\n",
    "print(myFeature['name'], myFeature['label'],\n",
    "      myFeature['m_features'].shape, \n",
    "      myFeature['v_features'].shape\n",
    "      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FL_MobileNet_ViT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
